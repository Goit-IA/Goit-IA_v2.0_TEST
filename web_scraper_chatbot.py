import sys
import time
import threading

from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain_ollama import OllamaEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- CONSTANTES DE CONFIGURACI√ìN ---
CHROMA_PATH = "chroma_db_web"
MODELO_OLLAMA = "phi3:mini"
MODELO_EMBEDDING = "nomic-embed-text"

# --- FUNCI√ìN DE CONFIGURACI√ìN DE LA CADENA RAG ---
# (CAMBIO) Se elimin√≥ el bloque try/except para que los errores se propaguen hacia arriba.
def get_rag_chain():
    """
    Configura y devuelve la cadena de RAG (Retrieval-Augmented Generation) completa.
    Si ocurre un error durante la configuraci√≥n (ej. no se encuentra Chroma),
    la excepci√≥n ser√° lanzada para que la funci√≥n que llama la maneje.
    """
    embeddings = OllamaEmbeddings(model=MODELO_EMBEDDING)
    
    # Esta es la l√≠nea que probablemente podr√≠a fallar si la carpeta no existe.
    vectorstore = Chroma(
        persist_directory=CHROMA_PATH, 
        embedding_function=embeddings
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={'k': 4})

    template ="""
    Act√∫a como un asistente virtual experto y muy servicial de la Universidad Veracruzana. 
    Tu misi√≥n es proporcionar respuestas extremadamente detalladas y completas, utilizando √∫nicamente la informaci√≥n encontrada en el CONTEXTO proporcionado.

    Sigue estas reglas estrictamente:
    1.  **S√© Exhaustivo:** Extrae y sintetiza TODA la informaci√≥n relevante del contexto que responda a la pregunta del usuario. No omitas detalles, requisitos, fechas o pasos mencionados.
    2.  **Organiza la Informaci√≥n:** Estructura tu respuesta de una manera clara y f√°cil de entender. Si la pregunta es sobre un proceso, descr√≠belo en una lista ordenada (paso a paso). Si se listan requisitos, usa vi√±etas.
    3.  **Elabora la Respuesta:** No te limites a extraer texto. Explica los conceptos con tus propias palabras (basadas en el contexto) para que la respuesta sea coherente y completa. El objetivo es que el usuario entienda el tema a fondo.
    4.  **Restricci√≥n Absoluta:** Si la informaci√≥n necesaria para responder la pregunta no se encuentra en el CONTEXTO, DEBES responder √∫nica y exclusivamente con la frase: "No tengo informaci√≥n suficiente sobre eso en mis documentos." No intentes adivinar ni a√±adir informaci√≥n externa.

    ---
    CONTEXTO:
    {context}
    ---
    PREGUNTA DEL USUARIO:
    {question}
    ---

    RESPUESTA DETALLADA Y COMPLETA:
    """
    prompt = ChatPromptTemplate.from_template(template)
    llm = Ollama(model=MODELO_OLLAMA)

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# --- FUNCI√ìN PARA LA ANIMACI√ìN DE CARGA --- (Sin cambios)
def mostrar_barra_de_carga(evento_parada):
    """
    Muestra una barra de progreso animada en la consola hasta que
    el 'evento_parada' se active.
    """
    bar_length = 30
    i = 0
    while not evento_parada.is_set():
        progreso = i % 100
        longitud_llena = int(bar_length * progreso // 100)
        barra = '‚ñà' * longitud_llena + '-' * (bar_length - longitud_llena)
        sys.stdout.write(f'\rChatbot: Procesando... |{barra}| {progreso}%')
        sys.stdout.flush()
        i += 2
        time.sleep(0.1)

    barra_final = '‚ñà' * bar_length
    sys.stdout.write(f'\rChatbot: Procesando... |{barra_final}| 100%\n')
    sys.stdout.flush()

# --- FUNCI√ìN PRINCIPAL ---
def main():
    """
    Funci√≥n principal que inicia el chatbot interactivo.
    """
    print("ü§ñ Iniciando chatbot con informaci√≥n web...")
    
    # (CAMBIO CLAVE) Se a√±ade un bloque try/except para capturar errores de inicializaci√≥n.
    try:
        rag_chain = get_rag_chain()
        print("\n‚úÖ Chatbot listo. Preg√∫ntame sobre los tr√°mites de la UV. Escribe 'salir' para terminar.")
        print("-" * 70)
    except Exception as e:
        print("\n‚ùå ERROR CR√çTICO AL INICIAR EL CHATBOT ‚ùå")
        print(f"No se pudo cargar la cadena de procesamiento de lenguaje. El error fue:\n")
        print(f"   ‚û°Ô∏è  {e}\n")
        print("POSIBLES CAUSAS:")
        print(f"   1. La carpeta de la base de datos Chroma ('{CHROMA_PATH}') no existe o est√° corrupta.")
        print("      Aseg√∫rate de haber ejecutado primero el script que la crea (ej. 'web_scraper_vectordb.py').")
        print(f"   2. El modelo de Ollama ('{MODELO_OLLAMA}' o '{MODELO_EMBEDDING}') no est√° disponible o no se ha descargado.")
        print("      Verifica que Ollama est√© en ejecuci√≥n y los modelos est√©n instalados con 'ollama list'.")
        sys.exit(1) # Terminamos el programa de forma controlada porque no puede funcionar.

    while True:
        pregunta = input("T√∫: ")
        
        if pregunta.lower() == 'salir':
            print("\nü§ñ ¬°Hasta luego! Ha sido un placer ayudarte.")
            break
        
        evento_parada = threading.Event()
        resultado = {"respuesta": None, "error": None}

        def obtener_respuesta(p):
            try:
                respuesta = rag_chain.invoke(p)
                resultado["respuesta"] = respuesta
            except Exception as e:
                resultado["error"] = e
            finally:
                evento_parada.set()

        hilo_trabajo = threading.Thread(target=obtener_respuesta, args=(pregunta,))
        hilo_trabajo.start()

        mostrar_barra_de_carga(evento_parada)

        hilo_trabajo.join()

        if resultado["error"]:
            # Este mensaje ahora es para errores DURANTE la conversaci√≥n.
            print(f"\nLo siento, ocurri√≥ un error al procesar tu pregunta.")
            print(f"Detalle del error: {resultado['error']}")
        else:
            print(f"Chatbot: {resultado['respuesta']}")

if __name__ == "__main__":
    main()