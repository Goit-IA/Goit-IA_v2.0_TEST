import sys
import time      # <--- Â¡NUEVO! Para controlar la velocidad de la animaciÃ³n
import threading # <--- Â¡NUEVO! Para ejecutar la carga en segundo plano

from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain_ollama import OllamaEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- CONSTANTES DE CONFIGURACIÃ“N ---
CHROMA_PATH = "chroma_db_web"
MODELO_OLLAMA = "phi3:mini"
MODELO_EMBEDDING = "nomic-embed-text"

def get_rag_chain():
    """
    Configura y devuelve la cadena de RAG (Retrieval-Augmented Generation) completa.
    """
    try:
        embeddings = OllamaEmbeddings(model=MODELO_EMBEDDING)
        vectorstore = Chroma(
            persist_directory=CHROMA_PATH, 
            embedding_function=embeddings
        )
        
        retriever = vectorstore.as_retriever(search_kwargs={'k': 4})

        template ="""
        ActÃºa como un asistente virtual experto y muy servicial de la Universidad Veracruzana. 
        Tu misiÃ³n es proporcionar respuestas extremadamente detalladas y completas, utilizando Ãºnicamente la informaciÃ³n encontrada en el CONTEXTO proporcionado.

        Sigue estas reglas estrictamente:
        1.  **SÃ© Exhaustivo:** Extrae y sintetiza TODA la informaciÃ³n relevante del contexto que responda a la pregunta del usuario. No omitas detalles, requisitos, fechas o pasos mencionados.
        2.  **Organiza la InformaciÃ³n:** Estructura tu respuesta de una manera clara y fÃ¡cil de entender. Si la pregunta es sobre un proceso, descrÃ­belo en una lista ordenada (paso a paso). Si se listan requisitos, usa viÃ±etas.
        3.  **Elabora la Respuesta:** No te limites a extraer texto. Explica los conceptos con tus propias palabras (basadas en el contexto) para que la respuesta sea coherente y completa. El objetivo es que el usuario entienda el tema a fondo.
        4.  **RestricciÃ³n Absoluta:** Si la informaciÃ³n necesaria para responder la pregunta no se encuentra en el CONTEXTO, DEBES responder Ãºnica y exclusivamente con la frase: "No tengo informaciÃ³n suficiente sobre eso en mis documentos." No intentes adivinar ni aÃ±adir informaciÃ³n externa.

        ---
        CONTEXTO:
        {context}
        ---
        PREGUNTA DEL USUARIO:
        {question}
        ---

        RESPUESTA DETALLADA Y COMPLETA:
        """
        prompt = ChatPromptTemplate.from_template(template)
        llm = Ollama(model=MODELO_OLLAMA)

        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain

    except Exception as e:
        print(f"OcurriÃ³ un error inesperado al cargar la cadena RAG: {e}")
        print(f"AsegÃºrate de haber ejecutado primero el script 'web_scraper_vectordb.py' y que la carpeta '{CHROMA_PATH}' exista.")
        sys.exit(1)

# --- FUNCIÃ“N PARA LA ANIMACIÃ“N DE CARGA --- (Â¡NUEVO!)
def mostrar_barra_de_carga(evento_parada):
    """
    Muestra una barra de progreso animada en la consola hasta que
    el 'evento_parada' se active.
    """
    bar_length = 30
    i = 0
    # Mientras el evento no se haya activado (la tarea principal no ha terminado)
    while not evento_parada.is_set():
        # Calculamos un porcentaje que va de 0 a 99 y vuelve a empezar
        # Esto da una sensaciÃ³n de progreso continuo
        progreso = i % 100
        longitud_llena = int(bar_length * progreso // 100)
        barra = 'â–ˆ' * longitud_llena + '-' * (bar_length - longitud_llena)
        
        # El carÃ¡cter \r al principio mueve el cursor al inicio de la lÃ­nea
        # para sobreescribirla y crear la animaciÃ³n
        sys.stdout.write(f'\rChatbot: Procesando... |{barra}| {progreso}%')
        sys.stdout.flush()
        
        i += 2
        time.sleep(0.1)

    # Cuando la tarea termina, borramos la barra y mostramos el 100%
    barra_final = 'â–ˆ' * bar_length
    sys.stdout.write(f'\rChatbot: Procesando... |{barra_final}| 100%\n')
    sys.stdout.flush()


def main():
    """
    FunciÃ³n principal que inicia el chatbot interactivo.
    """
    print("ðŸ¤– Iniciando chatbot con informaciÃ³n web... (Esto puede tardar un momento)")
    rag_chain = get_rag_chain()
    
    print("\nâœ… Chatbot listo. PregÃºntame sobre los trÃ¡mites de la UV. Escribe 'salir' para terminar.")
    print("-" * 70)

    while True:
        pregunta = input("TÃº: ")
        
        if pregunta.lower() == 'salir':
            print("\nðŸ¤– Â¡Hasta luego! Ha sido un placer ayudarte.")
            break
        
        # --- LÃ“GICA DE CARGA MODIFICADA --- (Â¡CAMBIO!)

        # 1. Preparar las herramientas para la comunicaciÃ³n entre hilos
        evento_parada = threading.Event()
        resultado = {"respuesta": None, "error": None}

        # 2. Definir la funciÃ³n que harÃ¡ el trabajo pesado (la llamada al LLM)
        def obtener_respuesta(p):
            try:
                respuesta = rag_chain.invoke(p)
                resultado["respuesta"] = respuesta
            except Exception as e:
                resultado["error"] = e
            finally:
                # 3. Avisar al hilo principal que ya hemos terminado
                evento_parada.set()

        # 4. Crear y empezar el hilo que obtendrÃ¡ la respuesta
        hilo_trabajo = threading.Thread(target=obtener_respuesta, args=(pregunta,))
        hilo_trabajo.start()

        # 5. Mientras el hilo de trabajo se ejecuta, mostramos la animaciÃ³n en el hilo principal
        mostrar_barra_de_carga(evento_parada)

        # 6. Una vez que la animaciÃ³n termina (porque el hilo de trabajo la detuvo),
        # esperamos a que el hilo termine de limpiarse y mostramos el resultado.
        hilo_trabajo.join()

        if resultado["error"]:
            print(f"\nOcurriÃ³ un error al procesar la pregunta: {resultado['error']}")
        else:
            print(f"Chatbot: {resultado['respuesta']}")


if __name__ == "__main__":
    main()